Question-1

Task1:
In Task in all of the csv files were combined to create a new text file. Here pandas library was used.

<img width="1196" alt="Screenshot 2024-01-09 at 12 59 45 PM" src="https://github.com/Sonish1212/Software-now-assignment-2/assets/69333078/1a05e1fd-e3e5-405d-9dec-f48642d078ea">

Task2:

Here creating a function the text file was read pre processed and top 30 words were counted and then using csv library, csv file was created to stored top 30 words
<img width="1102" alt="Screenshot 2024-01-09 at 1 07 47 PM" src="https://github.com/Sonish1212/Software-now-assignment-2/assets/69333078/1e59eecb-c618-4972-a117-94b1766981b6">

Task 3:

In Task 3, The provided Python function, tokenize_and_count, appears to use the Hugging Face Transformers library for tokenization and aims to tokenize a given text chunk and count the occurrences of each token using the Counter class from Python's collections module.
then,
read_chunks, reads a file in chunks of a specified size and yields each chunk as it's read.
and here using pool process executor, count_and_display_top_tokens function tokenize and count the apperanc of token

<img width="1388" alt="Screenshot 2024-01-17 at 12 57 22 AM" src="https://github.com/Sonish1212/Software-now-assignment-2/assets/69333078/9bbedfcb-0f83-4c8f-8d13-734019ec07e4">

Task-4:

In task 4 using biobert and en_core_sci_sm we have to understand how they extract the drugs and disease entities since biobert is used for medical term the output was easier for me to extract it takes less amount of time but on other hand the memory was insufficient for extracting entities using en_core_sci_sm so result couldnot been extraxted just it keep showing message killed:9
Using Biobert
<img width="1388" alt="Screenshot 2024-01-14 at 2 05 30 PM" src="https://github.com/Sonish1212/Software-now-assignment-2/assets/69333078/b62d6108-360f-4c2c-bcf5-25b3e5c046df">


